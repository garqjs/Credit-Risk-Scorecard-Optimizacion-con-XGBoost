{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRHq7de5Zfgk"
      },
      "outputs": [],
      "source": [
        "# ExtracciÃ³n: ConexiÃ³n a Drive y DuckDB.\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Montar Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_path = '/content/drive/My Drive/Colab Notebooks/PredicciÃ³n de crÃ©dito/'\n",
        "\n",
        "application = base_path + 'application_train.csv'\n",
        "bureau = base_path + 'bureau.csv'\n",
        "payments = base_path + 'installments_payments.csv'\n",
        "previous = base_path + 'previous_application.csv'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TransformaciÃ³n (SQL): Limpieza y agregaciÃ³n de las 4 fuentes (Vistas).\n",
        "\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ConexiÃ³n en memoria\n",
        "con = duckdb.connect(database=':memory:')\n",
        "\n",
        "# 1. BUREAU: Deuda Externa (Agregamos el conteo de prÃ©stamos activos)\n",
        "query_bureau = f\"\"\"\n",
        "CREATE OR REPLACE VIEW bureau_agg AS\n",
        "SELECT\n",
        "    SK_ID_CURR,\n",
        "    COUNT(*) as bureau_loan_count,\n",
        "    SUM(AMT_CREDIT_SUM_DEBT) as total_external_debt,\n",
        "    MAX(AMT_CREDIT_SUM_OVERDUE) as max_external_overdue,\n",
        "    SUM(CASE WHEN CREDIT_ACTIVE = 'Active' THEN 1 ELSE 0 END) as active_loan_count\n",
        "FROM read_csv_auto('{bureau}')\n",
        "GROUP BY SK_ID_CURR\n",
        "\"\"\"\n",
        "\n",
        "# 2. INSTALLMENTS: Voluntad de Pago (Mantenemos el ratio y dÃ­as de mora)\n",
        "query_install = f\"\"\"\n",
        "CREATE OR REPLACE VIEW install_agg AS\n",
        "SELECT\n",
        "    SK_ID_CURR,\n",
        "    AVG(AMT_PAYMENT / NULLIF(AMT_INSTALMENT, 0)) as avg_payment_ratio,\n",
        "    AVG(DAYS_ENTRY_PAYMENT - DAYS_INSTALMENT) as avg_days_late,\n",
        "    COUNT(*) as total_installments_done\n",
        "FROM read_csv_auto('{payments}')\n",
        "GROUP BY SK_ID_CURR\n",
        "\"\"\"\n",
        "\n",
        "# 3. PREVIOUS APP: Memoria Interna (La pieza que faltaba)\n",
        "# El ratio de rechazo es un predictor BRUTAL de default\n",
        "query_prev = f\"\"\"\n",
        "CREATE OR REPLACE VIEW prev_agg AS\n",
        "SELECT\n",
        "    SK_ID_CURR,\n",
        "    COUNT(*) as total_prev_apps,\n",
        "    AVG(CASE WHEN NAME_CONTRACT_STATUS = 'Refused' THEN 1 ELSE 0 END) as internal_rejection_rate,\n",
        "    SUM(AMT_ANNUITY) as total_prev_annuity\n",
        "FROM read_csv_auto('{previous}')\n",
        "GROUP BY SK_ID_CURR\n",
        "\"\"\"\n",
        "\n",
        "# 4. TABLA MAESTRA: UniÃ³n de las 4 fuentes\n",
        "query_master = f\"\"\"\n",
        "SELECT\n",
        "    app.SK_ID_CURR,\n",
        "    app.TARGET,\n",
        "    app.NAME_CONTRACT_TYPE,\n",
        "    app.AMT_INCOME_TOTAL,\n",
        "    app.AMT_CREDIT,\n",
        "    app.AMT_ANNUITY,\n",
        "    app.DAYS_BIRTH / -365.0 as age_years,\n",
        "    app.DAYS_EMPLOYED as days_employed_raw,\n",
        "    app.EXT_SOURCE_2,\n",
        "    app.EXT_SOURCE_3,\n",
        "\n",
        "    -- Bureau\n",
        "    COALESCE(b.total_external_debt, 0) as total_external_debt,\n",
        "    COALESCE(b.max_external_overdue, 0) as max_external_overdue, -- Added this line\n",
        "    COALESCE(b.active_loan_count, 0) as active_loan_count,\n",
        "\n",
        "    -- Pagos\n",
        "    COALESCE(i.avg_payment_ratio, 1.0) as avg_payment_ratio,\n",
        "    COALESCE(i.avg_days_late, 0) as avg_days_late,\n",
        "\n",
        "    -- Historial Interno\n",
        "    COALESCE(p.internal_rejection_rate, 0) as internal_rejection_rate,\n",
        "    COALESCE(p.total_prev_apps, 0) as total_prev_apps\n",
        "\n",
        "FROM read_csv_auto('{application}') app\n",
        "LEFT JOIN bureau_agg b ON app.SK_ID_CURR = b.SK_ID_CURR\n",
        "LEFT JOIN install_agg i ON app.SK_ID_CURR = i.SK_ID_CURR\n",
        "LEFT JOIN prev_agg p    ON app.SK_ID_CURR = p.SK_ID_CURR\n",
        "\"\"\"\n",
        "\n",
        "# --- AUDITORÃA (DQC) Y CALIDAD DE DATOS (Estilo Bancario) ---\n",
        "\n",
        "def run_audit(con, tables):\n",
        "    print(f\"{' REPORTE DE AUDITORÃA DE DATOS ':=^40}\")\n",
        "    for table in tables:\n",
        "        # Contamos nulos y volumen total por tabla\n",
        "        audit_query = f\"\"\"\n",
        "        SELECT\n",
        "            '{table}' as tabla,\n",
        "            COUNT(*) as total_registros,\n",
        "            SUM(CASE WHEN SK_ID_CURR IS NULL THEN 1 ELSE 0 END) as nulos_id,\n",
        "            APPROX_COUNT_DISTINCT(SK_ID_CURR) as clientes_unicos\n",
        "        FROM {table}\n",
        "        \"\"\"\n",
        "        res = con.execute(audit_query).df()\n",
        "        print(f\"ðŸ“Š Tabla: {res['tabla'][0]} | Registros: {res['total_registros'][0]} | Clientes Ãšnicos: {res['clientes_unicos'][0]}\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"{' FIN DEL REPORTE ':=^40}\")\n",
        "\n",
        "\n",
        "# Ejecutar el Pipeline\n",
        "con.execute(query_bureau)\n",
        "con.execute(query_install)\n",
        "con.execute(query_prev)\n",
        "df_master = con.execute(query_master).df()\n",
        "\n",
        "# --- AJUSTE DE LIMPIEZA ---\n",
        "# Primero: Limpiamos el error de sistema (365243 -> NaN)\n",
        "df_master['days_employed_raw'].replace(365243, np.nan, inplace=True)\n",
        "\n",
        "# Segundo: Creamos la variable final en aÃ±os positivos\n",
        "# El .abs() es vital para que la restricciÃ³n monotÃ³nica -1 funcione\n",
        "df_master['years_employed'] = df_master['days_employed_raw'].abs() / 365.0\n",
        "\n",
        "# Tercero: Rellenamos nulos con 0 (asumiendo que si no hay dato, no tiene antigÃ¼edad)\n",
        "df_master['years_employed'].fillna(0, inplace=True)\n",
        "\n",
        "print(f\"âœ… Â¡Ã‰xito! Dataset consolidado con ({df_master.shape[0]},{df_master.shape[1]})\")"
      ],
      "metadata": {
        "id": "kTLHy2TsaJ5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skew\n",
        "from scipy.stats.mstats import winsorize\n",
        "import numpy as np\n",
        "\n",
        "# 1. Limpieza de valores errÃ³neos conocidos\n",
        "# 365243 es un error comÃºn en este dataset para personas desempleadas o jubiladas\n",
        "# df_master['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True) # This column was removed in the previous step\n",
        "\n",
        "# 2. Aplicar Winsorize para \"capar\" outliers en el percentil 99\n",
        "# Esto evita que un multimillonario distorsione la escala del modelo\n",
        "cols_to_winsorize = ['AMT_INCOME_TOTAL', 'total_external_debt', 'AMT_CREDIT', 'AMT_ANNUITY']\n",
        "\n",
        "for col in cols_to_winsorize:\n",
        "    if col in df_master.columns:\n",
        "        # Ponemos el techo en el 99%, sin tocar el suelo (0%)\n",
        "        df_master[col] = winsorize(df_master[col], limits=[0, 0.01])\n",
        "\n",
        "# 3. Identificar AsimetrÃ­a Alta y aplicar Logaritmo\n",
        "# En finanzas, los montos suelen tener mucha asimetrÃ­a a la derecha\n",
        "numeric_cols = df_master.select_dtypes(include=[np.number]).columns\n",
        "threshold = 1.0  # Umbral de asimetrÃ­a\n",
        "\n",
        "for col in numeric_cols:\n",
        "    # Ignoramos el TARGET y variables de ID\n",
        "    if col in ['TARGET', 'SK_ID_CURR']: continue\n",
        "\n",
        "    current_skew = df_master[col].skew()\n",
        "\n",
        "    if abs(current_skew) > threshold:\n",
        "        # Verificamos que los valores sean positivos para el logaritmo\n",
        "        # Sumamos 1 (log1p) para manejar ceros de forma segura\n",
        "        if df_master[col].min() >= 0:\n",
        "            print(f\"ðŸš€ Transformando {col} | Skew original: {current_skew:.2f}\")\n",
        "            df_master[col] = np.log1p(df_master[col])\n",
        "        else:\n",
        "            print(f\"âš ï¸ {col} tiene asimetrÃ­a ({current_skew:.2f}) pero contiene negativos. Se queda igual.\")\n",
        "\n",
        "print(\"\\nâœ… Tratamiento de asimetrÃ­a completado.\")"
      ],
      "metadata": {
        "id": "pFwM9TTQdmma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enriquecimiento: InyecciÃ³n de los Ratios de Negocio (Leverage, Friction).\n",
        "\n",
        "# 1. Global Leverage (Evitamos divisiÃ³n por 0)\n",
        "df_master['RATIO_GLOBAL_LEVERAGE'] = (df_master['total_external_debt'] + df_master['AMT_CREDIT']) / (df_master['AMT_INCOME_TOTAL'] + 1)\n",
        "\n",
        "# 2. Payment Friction (DÃ­as tarde ponderados por deuda vencida externa)\n",
        "# Si tiene mora externa, penaliza x1000, mÃ¡s los dÃ­as de retraso interno\n",
        "df_master['SCORE_PAYMENT_FRICTION'] = df_master['avg_days_late'] + (np.where(df_master['max_external_overdue'] > 0, 100, 0))\n",
        "\n",
        "# 3. Annuity Burden (Carga Financiera Mensual)\n",
        "df_master['RATIO_ANNUITY_INCOME'] = df_master['AMT_ANNUITY'] / (df_master['AMT_INCOME_TOTAL'] / 12 + 1)\n",
        "\n",
        "# Limpieza de seguridad para los nuevos ratios\n",
        "cols_ratios = ['RATIO_GLOBAL_LEVERAGE', 'SCORE_PAYMENT_FRICTION', 'RATIO_ANNUITY_INCOME']\n",
        "\n",
        "# Reemplazar infinitos por el valor mÃ¡ximo y nulos por la mediana (o 0)\n",
        "for col in cols_ratios:\n",
        "    df_master[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df_master[col].fillna(df_master[col].median(), inplace=True)\n",
        "\n",
        "print(\"âœ… Ratios de negocio integrados y saneados.\")"
      ],
      "metadata": {
        "id": "5s1ImBMCeX86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- 1. SELECCIÃ“N DE FEATURES ESTRATÃ‰GICAS ---\n",
        "# Incluimos years_employed (estabilidad) y age_years (madurez)\n",
        "features = [\n",
        "    'AMT_INCOME_TOTAL',\n",
        "    'age_years',              # Edad del cliente (AÃ±os +)\n",
        "    'years_employed',         # AntigÃ¼edad laboral (AÃ±os +)\n",
        "    'EXT_SOURCE_2',\n",
        "    'EXT_SOURCE_3',\n",
        "    'total_external_debt',\n",
        "    'avg_payment_ratio',\n",
        "    'internal_rejection_rate',\n",
        "    'RATIO_GLOBAL_LEVERAGE',\n",
        "    'SCORE_PAYMENT_FRICTION',\n",
        "    'RATIO_ANNUITY_INCOME'\n",
        "]\n",
        "\n",
        "X = df_master[features]\n",
        "y = df_master['TARGET']\n",
        "\n",
        "# --- 2. SPLIT DE DATOS ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# --- 3. CONFIGURACIÃ“N DE RESTRICCIONES MONOTÃ“NICAS ---\n",
        "# -1: Si la variable sube, el riesgo BAJA.\n",
        "#  1: Si la variable sube, el riesgo SUBE.\n",
        "constraints = {\n",
        "    'AMT_INCOME_TOTAL': -1,\n",
        "    'age_years': -1,              # A mÃ¡s edad, estadÃ­sticamente menos riesgo\n",
        "    'years_employed': -1,         # A mÃ¡s antigÃ¼edad, menos riesgo\n",
        "    'EXT_SOURCE_2': -1,\n",
        "    'EXT_SOURCE_3': -1,\n",
        "    'total_external_debt': 1,\n",
        "    'avg_payment_ratio': -1,      # A mejor cumplimiento, menos riesgo\n",
        "    'internal_rejection_rate': 1, # MÃ¡s rechazos previos, mÃ¡s riesgo\n",
        "    'RATIO_GLOBAL_LEVERAGE': 1,   # MÃ¡s apalancamiento, mÃ¡s riesgo\n",
        "    'SCORE_PAYMENT_FRICTION': 1,  # MÃ¡s fricciÃ³n de pago, mÃ¡s riesgo\n",
        "    'RATIO_ANNUITY_INCOME': 1     # Mayor carga financiera, mÃ¡s riesgo\n",
        "}\n",
        "\n",
        "# Creamos la tupla respetando el orden exacto de las columnas en X_train\n",
        "constraint_tuple = tuple([constraints.get(col, 0) for col in X_train.columns])\n",
        "\n",
        "# --- 4. ENTRENAMIENTO DEL MODELO XGBOOST ---\n",
        "model = xgb.XGBClassifier(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=5,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    scale_pos_weight=11,          # CompensaciÃ³n por desbalanceo (clase 1 es escasa)\n",
        "    monotone_constraints=constraint_tuple,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    early_stopping_rounds=50      # Detiene el entrenamiento si el error de validaciÃ³n no baja\n",
        ")\n",
        "\n",
        "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=eval_set,\n",
        "    verbose=50\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Modelo entrenado con Ã©xito.\")"
      ],
      "metadata": {
        "id": "D1eB8pwbgiaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Extraer la importancia de las variables (usando 'gain' que es el mÃ¡s preciso)\n",
        "importance_type = 'gain'\n",
        "importance_scores = model.get_booster().get_score(importance_type=importance_type)\n",
        "\n",
        "# 2. Convertir a DataFrame para graficar\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': list(importance_scores.keys()),\n",
        "    'Importance': list(importance_scores.values())\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# 3. Crear el grÃ¡fico\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')\n",
        "\n",
        "plt.title(f'Importancia de Variables (MÃ©trica: {importance_type.capitalize()})')\n",
        "plt.xlabel('Ganancia Media (ContribuciÃ³n al Modelo)')\n",
        "plt.ylabel('Variables (Features)')\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# 4. Breve AnÃ¡lisis\n",
        "top_feature = importance_df.iloc[0]['Feature']\n",
        "print(f\"\\nâœ… La variable mÃ¡s influyente es: {top_feature}\")"
      ],
      "metadata": {
        "id": "wQermfGgk75l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "# 1. Crear el explicador (usando el modelo XGBoost ganador)\n",
        "# Using model_xgb from the benchmarking cell (GGzI98INXKgq) as 'model' is no longer defined.\n",
        "explainer = shap.TreeExplainer(model)\n",
        "\n",
        "# 2. Calcular los valores SHAP para el conjunto de test\n",
        "# Note: X_test here is the one from the most recent train_test_split (test_size=0.25) in GGzI98INXKgq\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# 3. VisualizaciÃ³n del Summary Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"dot\", show=False)\n",
        "#plt.title(\"Impacto SHAP: 4 Datasets + IngenierÃ­a de Ratios\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O_aZ1FRA7675"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. ObtenciÃ³n de Probabilidades\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 2. MÃ©tricas de Performance Core\n",
        "auc = roc_auc_score(y_test, y_probs)\n",
        "gini = 2 * auc - 1  # RelaciÃ³n directa Gini-AUC\n",
        "\n",
        "# 3. CÃ¡lculo de Curva KS, FPR y TPR\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "ks_values = tpr - fpr\n",
        "max_ks_index = np.argmax(ks_values)\n",
        "optimal_cutoff = thresholds[max_ks_index]\n",
        "ks_stat = ks_values[max_ks_index]\n",
        "\n",
        "# --- VISUALIZACIÃ“N 1: CURVA KS ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds, tpr, label='True Positive Rate (Sensibilidad)', color='green', lw=2)\n",
        "plt.plot(thresholds, fpr, label='False Positive Rate (1-Especificidad)', color='red', lw=2)\n",
        "plt.axvline(optimal_cutoff, color='black', linestyle='--', label=f'Corte Ã“ptimo: {optimal_cutoff:.3f}')\n",
        "plt.title(f'Curva KS (SeparaciÃ³n MÃ¡xima)\\nEstadÃ­stico KS: {ks_stat:.4f} | Gini: {gini:.4f}')\n",
        "plt.xlabel('Umbral de Probabilidad (Threshold)')\n",
        "plt.ylabel('Tasa Acumulada')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.gca().invert_xaxis() # El threshold suele ir de 1 a 0\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NFHhHT0jlEy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- VISUALIZACIÃ“N 2: MATRIZ DE CONFUSIÃ“N ---\n",
        "y_pred_optimized = (y_probs >= optimal_cutoff).astype(int)\n",
        "cm = confusion_matrix(y_test, y_pred_optimized)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Sano', 'Default'])\n",
        "disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
        "plt.title(f'Matriz de ConfusiÃ³n al Corte KS ({optimal_cutoff:.3f})')\n",
        "plt.show()\n",
        "\n",
        "# --- REPORTE DE IMPACTO Y MÃ‰TRICAS FINALES ---\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "tpr_final = tp / (tp + fn)\n",
        "fpr_final = fp / (fp + tn)\n",
        "\n",
        "print(f\"{' METRICAS FINALES ':=^40}\")\n",
        "print(f\"AUC Score: {auc:.4f}\")\n",
        "print(f\"Coeficiente Gini: {gini:.4f}\")\n",
        "print(f\"EstadÃ­stico KS: {ks_stat:.4f}\")\n",
        "print(f\"Threshold Seleccionado: {optimal_cutoff:.4f}\")\n",
        "print(f\"True Positive Rate (Atrapamos al % de morosos): {tpr_final:.2%}\")\n",
        "print(f\"False Positive Rate (Rechazamos al % de buenos): {fpr_final:.2%}\")\n",
        "\n",
        "print(f\"\\n{' REPORTE DE IMPACTO DE NEGOCIO ':=^40}\")\n",
        "print(f\"âœ… Clientes Buenos que generarÃ¡n intereses: {tn}\")\n",
        "print(f\"ðŸ›¡ï¸ Intentos de Default prevenidos con Ã©xito: {tp}\")\n",
        "print(f\"âš ï¸ Costo de Oportunidad (Buenos rechazados): {fp}\")\n",
        "print(f\"âŒ PÃ©rdida Directa (Morosos que se filtraron): {fn}\")"
      ],
      "metadata": {
        "id": "vBTcKKzcnQg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "# 1. PreparaciÃ³n de datos (Usando tu df_master real)\n",
        "# AsegÃºrate de haber ejecutado las celdas de limpieza y ratios antes\n",
        "X = df_master[features]\n",
        "y = df_master['TARGET']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# --- DEFINICIÃ“N DE MODELOS (Estrategias Diferentes) ---\n",
        "\n",
        "# A. RegresiÃ³n LogÃ­stica: El modelo lineal tradicional (Auditable pero rÃ­gido)\n",
        "pipe_lr = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('clf', LogisticRegression(class_weight='balanced', solver='liblinear', C=0.1))\n",
        "])\n",
        "\n",
        "# B. Random Forest: El ensamble robusto (Maneja no-linealidad)\n",
        "pipe_rf = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('clf', RandomForestClassifier(n_estimators=100, max_depth=8, class_weight='balanced', n_jobs=-1, random_state=42))\n",
        "])\n",
        "\n",
        "# C. XGBoost: El retador de alta precisiÃ³n (Maneja nulos y lÃ³gica monotÃ³nica)\n",
        "ratio_neg_pos = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "model_xgb = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=5,\n",
        "    scale_pos_weight=ratio_neg_pos,\n",
        "    monotone_constraints=constraint_tuple, # Aplicamos la lÃ³gica de negocio que definimos\n",
        "    eval_metric='logloss',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# --- ENTRENAMIENTO Y COMPETENCIA ---\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(f\"{' INICIANDO BENCHMARK ':=^40}\")\n",
        "for name, model in [('Logit Regression', pipe_lr), ('Random Forest', pipe_rf), ('XGBoost', model_xgb)]:\n",
        "    print(f\"ðŸš€ Entrenando {name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Probabilidades\n",
        "    preds = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # MÃ©tricas: AUC y Gini\n",
        "    auc = roc_auc_score(y_test, preds)\n",
        "    gini = 2 * auc - 1\n",
        "    results[name] = {'auc': auc, 'gini': gini, 'preds': preds}\n",
        "    print(f\"   >>> AUC: {auc:.4f} | Gini: {gini:.4f}\")\n",
        "\n",
        "# --- VISUALIZACIÃ“N DE RESULTADOS ---\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "for name, res in results.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, res['preds'])\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (Gini = {res['gini']:.3f})\", lw=2)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Modelo Azar (Gini = 0.0)')\n",
        "plt.xlabel('False Positive Rate (Riesgo de aceptar morosos)')\n",
        "plt.ylabel('True Positive Rate (Capacidad de detectar defaults)')\n",
        "plt.title('Comparativa de Modelos: Curvas ROC y Poder de DiscriminaciÃ³n')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.2)\n",
        "plt.show()\n",
        "\n",
        "# Resumen en Tabla\n",
        "summary = pd.DataFrame(results).T[['auc', 'gini']].sort_values(by='gini', ascending=False)\n",
        "print(\"\\nRANKING FINAL DE MODELOS:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "GGzI98INXKgq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}